#!/usr/bin/env python3
"""
🧪 COMPREHENSIVE END-TO-END TEST SUITE
TFT Trading System - Complete Workflow Validation

This test suite validates the entire TFT trading system from data ingestion 
to trade execution, ensuring all components work together correctly.

Test Categories:
- E2E Workflow Tests: Complete business process validation
- Performance Tests: System performance under load
- Integration Tests: Component interaction validation
- Data Quality Tests: Data pipeline validation
- Error Handling Tests: System resilience testing
"""

import pytest
import asyncio
import time
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from unittest.mock import Mock, patch
from concurrent.futures import ThreadPoolExecutor, as_completed
import yfinance as yf
from typing import Dict, List, Any
import json
import logging

# Configure logging for tests
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TFTEndToEndTestSuite:
    """
    Comprehensive End-to-End Test Suite for TFT Trading System
    
    This class implements all E2E test scenarios covering:
    - Complete trading workflow validation
    - Performance benchmarking  
    - Error handling and recovery
    - Data quality assurance
    - System integration testing
    """
    
    def __init__(self):
        self.test_config = {
            'base_urls': {
                'data_ingestion': 'http://localhost:8001',
                'sentiment_engine': 'http://localhost:8002', 
                'tft_predictor': 'http://localhost:8003',
                'trading_engine': 'http://localhost:8004',
                'orchestrator': 'http://localhost:8005'
            },
            'test_symbols': ['AAPL', 'GOOGL', 'MSFT'],
            'timeout': 30,
            'performance_thresholds': {
                'api_response_time': 0.1,  # 100ms
                'ml_inference_time': 0.05,  # 50ms
                'e2e_workflow_time': 5.0,   # 5 seconds
                'concurrent_requests': 50
            }
        }
        self.test_results = {}
        
    async def setup_test_environment(self):
        """Setup test environment with mock data and services"""
        logger.info("🔧 Setting up E2E test environment...")
        
        # Generate test market data
        self.test_data = self._generate_test_market_data()
        
        # Initialize service health checks
        await self._verify_service_health()
        
        logger.info("✅ Test environment setup complete")
    
    def _generate_test_market_data(self) -> Dict[str, pd.DataFrame]:
        """Generate realistic test market data for validation"""
        test_data = {}
        
        for symbol in self.test_config['test_symbols']:
            # Generate 30 days of hourly data
            dates = pd.date_range(
                start=datetime.now() - timedelta(days=30),
                end=datetime.now(),
                freq='1H'
            )
            
            # Create realistic OHLCV data with random walk
            np.random.seed(42)  # Reproducible results
            n_points = len(dates)
            base_price = 150.0
            
            # Generate price movements
            returns = np.random.randn(n_points) * 0.02  # 2% volatility
            price_series = base_price * np.exp(np.cumsum(returns))
            
            # Create OHLCV data
            data = pd.DataFrame({
                'timestamp': dates,
                'open': price_series,
                'high': price_series * (1 + np.random.exponential(0.005, n_points)),
                'low': price_series * (1 - np.random.exponential(0.005, n_points)),
                'close': price_series * (1 + np.random.randn(n_points) * 0.001),
                'volume': np.random.randint(10000, 100000, n_points),
                'symbol': symbol
            })
            
            test_data[symbol] = data
            
        return test_data
    
    async def _verify_service_health(self):
        """Verify all microservices are running and healthy"""
        health_checks = []
        
        for service, url in self.test_config['base_urls'].items():
            try:
                response = requests.get(f"{url}/health", timeout=5)
                if response.status_code == 200:
                    logger.info(f"✅ {service} service is healthy")
                    health_checks.append(True)
                else:
                    logger.warning(f"⚠️ {service} service returned status {response.status_code}")
                    health_checks.append(False)
            except Exception as e:
                logger.warning(f"❌ {service} service unreachable: {e}")
                health_checks.append(False)
        
        if not all(health_checks):
            logger.info("🔄 Some services unavailable - running in mock mode")
    
    # ==================== CORE E2E WORKFLOW TESTS ====================
    
    @pytest.mark.asyncio
    @pytest.mark.e2e
    async def test_complete_trading_workflow(self):
        """
        Test Case: Complete End-to-End Trading Workflow
        
        This test validates the entire trading process:
        1. Data Ingestion → Market data collection
        2. Sentiment Analysis → Social sentiment processing  
        3. Feature Engineering → Technical indicators
        4. TFT Prediction → ML model inference
        5. Trading Signals → Buy/sell decision
        6. Trade Execution → Order placement
        
        Success Criteria:
        - All steps complete without errors
        - Data flows correctly between services
        - Final trade decision is valid
        - Performance meets SLA requirements
        """
        
        logger.info("🧪 Starting Complete E2E Trading Workflow Test")
        test_symbol = 'AAPL'
        workflow_start_time = time.time()
        
        try:
            # Step 1: Data Ingestion Test
            logger.info("📊 Step 1: Testing Data Ingestion")
            ingestion_result = await self._test_data_ingestion(test_symbol)
            assert ingestion_result['status'] == 'success'
            assert ingestion_result['rows'] > 100  # Minimum data points
            
            # Step 2: Sentiment Analysis Test
            logger.info("💭 Step 2: Testing Sentiment Analysis")
            sentiment_result = await self._test_sentiment_analysis(test_symbol)
            assert sentiment_result['status'] == 'success'
            assert 'sentiment_score' in sentiment_result
            assert -1 <= sentiment_result['sentiment_score'] <= 1
            
            # Step 3: Feature Engineering Test
            logger.info("⚙️ Step 3: Testing Feature Engineering")
            features_result = await self._test_feature_engineering(test_symbol)
            assert features_result['status'] == 'success'
            assert features_result['feature_count'] >= 20  # Minimum features
            
            # Step 4: TFT Prediction Test
            logger.info("🔮 Step 4: Testing TFT Model Prediction")
            prediction_result = await self._test_tft_prediction(test_symbol)
            assert prediction_result['status'] == 'success'
            assert 'point_forecast' in prediction_result
            assert 'confidence' in prediction_result
            
            # Step 5: Trading Signal Generation Test
            logger.info("📈 Step 5: Testing Trading Signal Generation")
            signal_result = await self._test_trading_signals(prediction_result)
            assert signal_result['status'] == 'success'
            assert signal_result['signal'] in ['BUY', 'SELL', 'HOLD']
            
            # Step 6: Trade Execution Test (Mock)
            logger.info("💰 Step 6: Testing Trade Execution")
            execution_result = await self._test_trade_execution(signal_result)
            assert execution_result['status'] == 'success'
            
            # Validate overall workflow performance
            total_time = time.time() - workflow_start_time
            assert total_time < self.test_config['performance_thresholds']['e2e_workflow_time']
            
            self.test_results['complete_workflow'] = {
                'status': 'PASSED',
                'execution_time': total_time,
                'steps_completed': 6,
                'symbol': test_symbol
            }
            
            logger.info(f"✅ Complete E2E Workflow Test PASSED in {total_time:.2f}s")
            
        except Exception as e:
            self.test_results['complete_workflow'] = {
                'status': 'FAILED',
                'error': str(e),
                'execution_time': time.time() - workflow_start_time
            }
            logger.error(f"❌ Complete E2E Workflow Test FAILED: {e}")
            raise
    
    async def _test_data_ingestion(self, symbol: str) -> Dict:
        """Test data ingestion service"""
        try:
            # Use real yfinance data for testing
            ticker = yf.Ticker(symbol)
            data = ticker.history(period="30d", interval="1h")
            
            if data.empty:
                return {'status': 'failed', 'error': 'No data retrieved'}
            
            # Simulate data quality validation
            quality_score = len(data) / (30 * 24)  # Expected vs actual data points
            
            return {
                'status': 'success',
                'rows': len(data),
                'columns': len(data.columns),
                'quality_score': min(quality_score, 1.0),
                'symbol': symbol
            }
            
        except Exception as e:
            return {'status': 'failed', 'error': str(e)}
    
    async def _test_sentiment_analysis(self, symbol: str) -> Dict:
        """Test sentiment analysis service"""
        try:
            # Simulate sentiment analysis with mock Reddit data
            mock_comments = [
                f"{symbol} is going to the moon! 🚀",
                f"Bearish on {symbol}, expecting a drop",
                f"Neutral outlook for {symbol} this week",
                f"Great earnings report for {symbol}!",
                f"Market volatility affecting {symbol}"
            ]
            
            # Simple sentiment scoring (mock implementation)
            positive_words = ['moon', 'great', 'bullish', 'up', 'gain']
            negative_words = ['drop', 'bearish', 'down', 'loss', 'crash']
            
            sentiment_scores = []
            for comment in mock_comments:
                score = 0
                for word in positive_words:
                    if word in comment.lower():
                        score += 1
                for word in negative_words:
                    if word in comment.lower():
                        score -= 1
                sentiment_scores.append(score)
            
            avg_sentiment = np.mean(sentiment_scores) / 5  # Normalize to -1,1
            
            return {
                'status': 'success',
                'sentiment_score': avg_sentiment,
                'comments_analyzed': len(mock_comments),
                'confidence': 0.75,
                'symbol': symbol
            }
            
        except Exception as e:
            return {'status': 'failed', 'error': str(e)}
    
    async def _test_feature_engineering(self, symbol: str) -> Dict:
        """Test feature engineering process"""
        try:
            data = self.test_data[symbol]
            
            # Calculate technical indicators
            features = {}
            features['sma_20'] = data['close'].rolling(20).mean()
            features['sma_50'] = data['close'].rolling(50).mean()
            features['rsi'] = self._calculate_rsi(data['close'])
            features['volatility'] = data['close'].pct_change().rolling(20).std()
            features['volume_sma'] = data['volume'].rolling(20).mean()
            
            # Count non-null features
            feature_count = sum(1 for f in features.values() if not f.isna().all())
            
            return {
                'status': 'success',
                'feature_count': feature_count,
                'features_generated': list(features.keys()),
                'symbol': symbol
            }
            
        except Exception as e:
            return {'status': 'failed', 'error': str(e)}
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """Calculate Relative Strength Index"""
        delta = prices.diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        
        avg_gain = gain.rolling(period).mean()
        avg_loss = loss.rolling(period).mean()
        
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))
        
        return rsi
    
    async def _test_tft_prediction(self, symbol: str) -> Dict:
        """Test TFT model prediction service"""
        try:
            # Mock TFT prediction (in production, this would call the actual service)
            data = self.test_data[symbol]
            current_price = data['close'].iloc[-1]
            
            # Simulate TFT prediction with uncertainty
            base_prediction = current_price * (1 + np.random.randn() * 0.02)  # ±2% prediction
            confidence = np.random.uniform(0.6, 0.9)
            
            # Generate quantile predictions
            std_dev = current_price * 0.01
            quantiles = {
                'p10': base_prediction - 1.28 * std_dev,
                'p50': base_prediction,
                'p90': base_prediction + 1.28 * std_dev
            }
            
            return {
                'status': 'success',
                'point_forecast': base_prediction,
                'quantiles': quantiles,
                'confidence': confidence,
                'current_price': current_price,
                'symbol': symbol
            }
            
        except Exception as e:
            return {'status': 'failed', 'error': str(e)}
    
    async def _test_trading_signals(self, prediction_result: Dict) -> Dict:
        """Test trading signal generation"""
        try:
            current_price = prediction_result['current_price']
            predicted_price = prediction_result['point_forecast']
            confidence = prediction_result['confidence']
            
            # Generate trading signal based on prediction
            price_change_pct = (predicted_price - current_price) / current_price
            
            if price_change_pct > 0.02 and confidence > 0.7:
                signal = 'BUY'
                signal_strength = min(price_change_pct * confidence, 1.0)
            elif price_change_pct < -0.02 and confidence > 0.7:
                signal = 'SELL'  
                signal_strength = min(abs(price_change_pct) * confidence, 1.0)
            else:
                signal = 'HOLD'
                signal_strength = 0.5
            
            return {
                'status': 'success',
                'signal': signal,
                'signal_strength': signal_strength,
                'predicted_return': price_change_pct,
                'confidence': confidence
            }
            
        except Exception as e:
            return {'status': 'failed', 'error': str(e)}
    
    async def _test_trade_execution(self, signal_result: Dict) -> Dict:
        """Test trade execution (mock implementation)"""
        try:
            signal = signal_result['signal']
            signal_strength = signal_result['signal_strength']
            
            if signal == 'HOLD':
                return {
                    'status': 'success',
                    'action': 'NO_TRADE',
                    'reason': 'Signal below threshold'
                }
            
            # Mock trade execution
            trade_details = {
                'action': signal,
                'quantity': int(signal_strength * 100),  # Scale with signal strength
                'estimated_price': 150.0 + np.random.randn() * 0.5,
                'order_type': 'MARKET',
                'timestamp': datetime.now().isoformat()
            }
            
            return {
                'status': 'success',
                'trade_executed': True,
                'trade_details': trade_details
            }
            
        except Exception as e:
            return {'status': 'failed', 'error': str(e)}
    
    # ==================== PERFORMANCE TESTS ====================
    
    @pytest.mark.performance
    def test_api_latency_performance(self):
        """
        Test Case: API Response Time Performance
        
        Validates that all API endpoints meet latency requirements:
        - Health checks: < 50ms
        - Data queries: < 100ms
        - ML predictions: < 50ms
        """
        
        logger.info("⚡ Testing API Latency Performance")
        latency_results = {}
        
        # Test each service endpoint
        for service, base_url in self.test_config['base_urls'].items():
            try:
                start_time = time.time()
                response = requests.get(f"{base_url}/health", timeout=5)
                response_time = time.time() - start_time
                
                latency_results[service] = {
                    'response_time': response_time,
                    'status_code': response.status_code,
                    'meets_sla': response_time < self.test_config['performance_thresholds']['api_response_time']
                }
                
                logger.info(f"📊 {service}: {response_time*1000:.1f}ms")
                
            except Exception as e:
                latency_results[service] = {
                    'error': str(e),
                    'meets_sla': False
                }
        
        # Assert performance requirements
        successful_tests = [r for r in latency_results.values() if r.get('meets_sla', False)]
        performance_ratio = len(successful_tests) / len(latency_results)
        
        assert performance_ratio >= 0.8, f"Only {performance_ratio:.1%} of services meet latency SLA"
        
        self.test_results['api_latency'] = latency_results
        logger.info(f"✅ API Latency Test: {performance_ratio:.1%} services meet SLA")
    
    @pytest.mark.performance
    def test_concurrent_load_performance(self):
        """
        Test Case: Concurrent Load Performance
        
        Tests system behavior under concurrent load:
        - 50 concurrent requests
        - Response time degradation
        - Error rate under load
        """
        
        logger.info("🔄 Testing Concurrent Load Performance")
        
        def make_request(request_id: int) -> Dict:
            try:
                start_time = time.time()
                # Test the TFT predictor service (most resource-intensive)
                response = requests.get(
                    f"{self.test_config['base_urls']['tft_predictor']}/health",
                    timeout=10
                )
                response_time = time.time() - start_time
                
                return {
                    'request_id': request_id,
                    'success': response.status_code == 200,
                    'response_time': response_time,
                    'status_code': response.status_code
                }
            except Exception as e:
                return {
                    'request_id': request_id,
                    'success': False,
                    'error': str(e),
                    'response_time': None
                }
        
        # Execute concurrent requests
        concurrent_requests = self.test_config['performance_thresholds']['concurrent_requests']
        with ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
            futures = [executor.submit(make_request, i) for i in range(concurrent_requests)]
            results = [future.result() for future in as_completed(futures)]
        
        # Analyze results
        successful_requests = [r for r in results if r['success']]
        success_rate = len(successful_requests) / len(results)
        
        if successful_requests:
            response_times = [r['response_time'] for r in successful_requests if r['response_time']]
            avg_response_time = np.mean(response_times) if response_times else 0
            p95_response_time = np.percentile(response_times, 95) if response_times else 0
        else:
            avg_response_time = 0
            p95_response_time = 0
        
        # Performance assertions
        assert success_rate >= 0.95, f"Success rate {success_rate:.1%} below 95% threshold"
        assert p95_response_time < 0.5, f"P95 response time {p95_response_time:.2f}s exceeds 500ms threshold"
        
        self.test_results['concurrent_load'] = {
            'total_requests': len(results),
            'successful_requests': len(successful_requests),
            'success_rate': success_rate,
            'avg_response_time': avg_response_time,
            'p95_response_time': p95_response_time
        }
        
        logger.info(f"✅ Concurrent Load Test: {success_rate:.1%} success rate, {p95_response_time*1000:.1f}ms P95")
    
    # ==================== DATA QUALITY TESTS ====================
    
    @pytest.mark.data_quality
    def test_data_pipeline_quality(self):
        """
        Test Case: Data Pipeline Quality Validation
        
        Ensures data quality throughout the pipeline:
        - Data completeness
        - Data accuracy
        - Data consistency
        - Data timeliness
        """
        
        logger.info("📊 Testing Data Pipeline Quality")
        quality_results = {}
        
        for symbol in self.test_config['test_symbols']:
            data = self.test_data[symbol]
            
            # Test data completeness
            completeness = 1 - (data.isnull().sum().sum() / (len(data) * len(data.columns)))
            
            # Test data accuracy (basic sanity checks)
            accuracy_checks = {
                'positive_prices': (data[['open', 'high', 'low', 'close']] > 0).all().all(),
                'high_low_consistency': (data['high'] >= data['low']).all(),
                'ohlc_consistency': (
                    (data['high'] >= data[['open', 'close']].max(axis=1)).all() and
                    (data['low'] <= data[['open', 'close']].min(axis=1)).all()
                ),
                'positive_volume': (data['volume'] >= 0).all()
            }
            accuracy = sum(accuracy_checks.values()) / len(accuracy_checks)
            
            # Test data consistency (no extreme outliers)
            price_changes = data['close'].pct_change().abs()
            consistency = (price_changes < 0.2).sum() / len(price_changes)  # Less than 20% moves
            
            # Test data timeliness (recent data available)
            latest_timestamp = data['timestamp'].max()
            timeliness = (datetime.now() - latest_timestamp).days < 1
            
            quality_score = (completeness + accuracy + consistency + timeliness) / 4
            
            quality_results[symbol] = {
                'completeness': completeness,
                'accuracy': accuracy,
                'consistency': consistency,
                'timeliness': timeliness,
                'overall_quality': quality_score,
                'passes_threshold': quality_score > 0.8
            }
        
        # Assert quality thresholds
        passing_symbols = sum(1 for r in quality_results.values() if r['passes_threshold'])
        quality_ratio = passing_symbols / len(quality_results)
        
        assert quality_ratio >= 0.8, f"Only {quality_ratio:.1%} of symbols meet quality threshold"
        
        self.test_results['data_quality'] = quality_results
        logger.info(f"✅ Data Quality Test: {quality_ratio:.1%} of symbols pass quality checks")
    
    # ==================== ERROR HANDLING TESTS ====================
    
    @pytest.mark.error_handling
    async def test_error_recovery_scenarios(self):
        """
        Test Case: Error Handling and Recovery
        
        Tests system resilience under various error conditions:
        - Network failures
        - Service timeouts
        - Data corruption
        - Resource exhaustion
        """
        
        logger.info("🛡️ Testing Error Recovery Scenarios")
        error_scenarios = {}
        
        # Scenario 1: Network timeout simulation
        try:
            start_time = time.time()
            requests.get("http://localhost:9999/nonexistent", timeout=0.1)
        except requests.exceptions.Timeout:
            error_scenarios['timeout_handling'] = {
                'handled_correctly': True,
                'response_time': time.time() - start_time
            }
        except Exception as e:
            error_scenarios['timeout_handling'] = {
                'handled_correctly': False,
                'error': str(e)
            }
        
        # Scenario 2: Invalid data input
        try:
            invalid_data = pd.DataFrame({'invalid': [np.nan, np.inf, -np.inf]})
            result = await self._test_data_validation(invalid_data)
            error_scenarios['invalid_data_handling'] = {
                'handled_correctly': result['status'] == 'failed',
                'error_detected': True
            }
        except Exception as e:
            error_scenarios['invalid_data_handling'] = {
                'handled_correctly': True,
                'error_caught': str(e)
            }
        
        # Scenario 3: Service unavailability
        error_scenarios['service_unavailability'] = {
            'fallback_implemented': True,  # Mock fallback behavior
            'graceful_degradation': True
        }
        
        # Assert error handling requirements
        handled_correctly = sum(1 for s in error_scenarios.values() 
                              if s.get('handled_correctly', False))
        error_handling_ratio = handled_correctly / len(error_scenarios)
        
        assert error_handling_ratio >= 0.8, f"Only {error_handling_ratio:.1%} of error scenarios handled correctly"
        
        self.test_results['error_handling'] = error_scenarios
        logger.info(f"✅ Error Handling Test: {error_handling_ratio:.1%} scenarios handled correctly")
    
    async def _test_data_validation(self, data: pd.DataFrame) -> Dict:
        """Test data validation logic"""
        try:
            # Check for invalid values
            if data.isnull().any().any():
                return {'status': 'failed', 'reason': 'null_values_detected'}
            if np.isinf(data.select_dtypes(include=[np.number]).values).any():
                return {'status': 'failed', 'reason': 'infinite_values_detected'}
            
            return {'status': 'success'}
            
        except Exception as e:
            return {'status': 'failed', 'error': str(e)}
    
    # ==================== TEST REPORTING ====================
    
    def generate_test_report(self) -> Dict:
        """Generate comprehensive test execution report"""
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results.values() 
                          if isinstance(r, dict) and r.get('status') == 'PASSED')
        
        report = {
            'test_execution_summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': total_tests - passed_tests,
                'success_rate': passed_tests / total_tests if total_tests > 0 else 0,
                'execution_timestamp': datetime.now().isoformat()
            },
            'test_categories': {
                'e2e_workflow': 'complete_workflow' in self.test_results,
                'performance': any(k in self.test_results for k in ['api_latency', 'concurrent_load']),
                'data_quality': 'data_quality' in self.test_results,
                'error_handling': 'error_handling' in self.test_results
            },
            'detailed_results': self.test_results,
            'recommendations': self._generate_recommendations()
        }
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on test results"""
        recommendations = []
        
        if 'api_latency' in self.test_results:
            slow_services = [service for service, result in self.test_results['api_latency'].items()
                           if not result.get('meets_sla', True)]
            if slow_services:
                recommendations.append(f"Optimize response time for services: {', '.join(slow_services)}")
        
        if 'data_quality' in self.test_results:
            low_quality_symbols = [symbol for symbol, result in self.test_results['data_quality'].items()
                                 if not result.get('passes_threshold', True)]
            if low_quality_symbols:
                recommendations.append(f"Improve data quality for symbols: {', '.join(low_quality_symbols)}")
        
        if not recommendations:
            recommendations.append("All tests passed - system performance meets requirements")
        
        return recommendations


# ==================== PYTEST INTEGRATION ====================

@pytest.fixture(scope="session")
def e2e_test_suite():
    """Fixture to initialize E2E test suite"""
    suite = TFTEndToEndTestSuite()
    asyncio.run(suite.setup_test_environment())
    return suite

@pytest.mark.e2e
@pytest.mark.asyncio
async def test_complete_workflow(e2e_test_suite):
    """Run complete E2E workflow test"""
    await e2e_test_suite.test_complete_trading_workflow()

@pytest.mark.performance
def test_api_latency(e2e_test_suite):
    """Run API latency performance test"""
    e2e_test_suite.test_api_latency_performance()

@pytest.mark.performance 
def test_concurrent_load(e2e_test_suite):
    """Run concurrent load test"""
    e2e_test_suite.test_concurrent_load_performance()

@pytest.mark.data_quality
def test_data_quality(e2e_test_suite):
    """Run data quality validation test"""
    e2e_test_suite.test_data_pipeline_quality()

@pytest.mark.error_handling
@pytest.mark.asyncio
async def test_error_handling(e2e_test_suite):
    """Run error handling and recovery test"""
    await e2e_test_suite.test_error_recovery_scenarios()

# ==================== MAIN EXECUTION ====================

async def main():
    """Main function to run all E2E tests"""
    print("🧪 TFT TRADING SYSTEM - END-TO-END TEST SUITE")
    print("=" * 60)
    
    # Initialize test suite
    suite = TFTEndToEndTestSuite()
    await suite.setup_test_environment()
    
    # Execute all test categories
    try:
        print("\n📊 Running Complete Workflow Test...")
        await suite.test_complete_trading_workflow()
        
        print("\n⚡ Running Performance Tests...")
        suite.test_api_latency_performance()
        suite.test_concurrent_load_performance()
        
        print("\n📈 Running Data Quality Tests...")
        suite.test_data_pipeline_quality()
        
        print("\n🛡️ Running Error Handling Tests...")
        await suite.test_error_recovery_scenarios()
        
    except Exception as e:
        print(f"❌ Test execution failed: {e}")
    
    # Generate and display report
    print("\n📋 GENERATING TEST REPORT...")
    report = suite.generate_test_report()
    
    print(f"\n✅ TEST EXECUTION COMPLETE")
    print(f"Success Rate: {report['test_execution_summary']['success_rate']:.1%}")
    print(f"Total Tests: {report['test_execution_summary']['total_tests']}")
    print(f"Passed: {report['test_execution_summary']['passed_tests']}")
    print(f"Failed: {report['test_execution_summary']['failed_tests']}")
    
    if report['recommendations']:
        print(f"\n💡 RECOMMENDATIONS:")
        for rec in report['recommendations']:
            print(f"  • {rec}")
    
    return report

if __name__ == "__main__":
    asyncio.run(main())
