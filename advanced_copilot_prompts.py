"""
ðŸš€ ADVANCED COPILOT PROMPTS FOR INSTITUTIONAL TRADING
====================================================

Your enhanced Copilot is ready for professional quantitative development.
Copy these prompts into your files and watch it generate production-grade code!

IMMEDIATE ACTION: Open VS Code and try these prompts!
"""

# ====================================================================
# ðŸ’° LIVE TRADING EXECUTION - Use in scheduler.py
# ====================================================================

def execute_live_trades():
    """
    Professional live trading execution with Alpaca integration
    Implements institutional risk controls and compliance logging
    """
    # Initialize Alpaca trading client with API credentials from environment
    
    # Fetch latest TFT predictions from PostgreSQL predictions table
    
    # Apply position sizing: max 5% portfolio per trade, sector limits 30%
    
    # Calculate stop-loss (2% below entry) and take-profit (4% above entry)
    
    # Check market conditions: skip if VIX > 35 or low liquidity
    
    # Execute market orders with slippage protection (0.1% max)
    
    # Handle partial fills with time-weighted average execution
    
    # Log all trades to PostgreSQL with tax lot tracking
    
    # Send Slack notifications for executions and failures
    
    # Return execution summary with P&L attribution

# ====================================================================
# ðŸ“Š EARNINGS IMPACT MODEL - Use in tft_postgres_model.py
# ====================================================================

def incorporate_earnings_surprise():
    """
    Advanced earnings integration with surprise momentum
    Enhances TFT predictions with fundamental event risk
    """
    # Fetch earnings calendar from Polygon API for next 30 days
    
    # Calculate historical earnings surprise percentage (actual vs estimate)
    
    # Create feature: days_until_next_earnings (0-90 day window)
    
    # Compute earnings surprise momentum (3-quarter trend analysis)
    
    # Add pre-earnings volatility expansion factor
    
    # Weight TFT predictions by earnings proximity (higher weight = higher uncertainty)
    
    # Implement earnings whisper number integration
    
    # Return enhanced model with earnings-aware predictions

# ====================================================================
# ðŸ” DARK POOL ANALYTICS - Use in enhanced_data_pipeline.py  
# ====================================================================

def process_dark_pool_signals():
    """  
    Institutional dark pool activity detection and analysis
    Identifies large block trades and unusual OTC activity
    """
    # Connect to Polygon dark pool data feeds
    
    # Calculate abnormal OTC volume ratio vs normal trading volume
    
    # Detect block trades >$1M using FINRA OTC data
    
    # Compute dark pool activity concentration by venue
    
    # Generate alerts for unusual institutional activity (>3 sigma)
    
    # Cross-reference with options flow for gamma hedging detection
    
    # Create dark pool momentum indicator (5-day rolling sum)
    
    # Store signals in PostgreSQL dark_pool_activity table
    
    # Return DataFrame with institutional flow indicators

# ====================================================================
# ðŸ’¸ TAX-OPTIMIZED TRADING - Use in stock_ranking.py
# ====================================================================

def optimize_tax_efficiency():
    """
    Tax-aware portfolio management with loss harvesting
    Maximizes after-tax returns while avoiding wash sales
    """
    # Track holding periods for all tax lots in PostgreSQL
    
    # Prioritize long-term capital gains (>365 days holding)
    
    # Implement tax loss harvesting: sell losers, avoid wash sales
    
    # Calculate wash sale violation risk (30-day substantially identical)
    
    # Optimize trade timing around year-end for tax benefits
    
    # Generate IRS Form 8949 export for tax reporting
    
    # Add municipal bond integration for high-tax-bracket clients
    
    # Return tax-optimized portfolio with after-tax Sharpe ratio

# ====================================================================
# âš¡ GPU ACCELERATION - Use in tft_postgres_model.py
# ====================================================================

def enable_cuda_optimization():
    """
    Production GPU acceleration with mixed-precision training
    Reduces model latency from 500ms to <50ms per prediction
    """
    # Initialize CUDA device and memory management
    
    # Implement mixed-precision training with automatic loss scaling
    
    # Add gradient checkpointing to reduce memory usage
    
    # Use torch.compile() with max-autotune for inference optimization
    
    # Optimize data loader with pinned memory and prefetching
    
    # Implement model quantization (INT8) for deployment
    
    # Add ONNX Runtime GPU provider for cross-platform deployment
    
    # Benchmark latency improvements and memory usage
    
    # Return optimized model with <50ms prediction latency

# ====================================================================
# ðŸ”„ ZERO-DOWNTIME DEPLOYMENT - Use in config_manager.py
# ====================================================================

def implement_hot_reload():
    """
    Blue-green deployment with automatic rollback capability
    Enables model updates without service interruption
    """
    # Create model version registry with semantic versioning
    
    # Implement shadow testing endpoint for A/B model comparison
    
    # Add canary deployment: route 10% traffic to new model
    
    # Monitor prediction drift using Kolmogorov-Smirnov test
    
    # Auto-rollback on performance degradation (>5% accuracy drop)
    
    # Implement feature flag system for gradual rollouts
    
    # Add health checks and circuit breaker patterns
    
    # Return deployment manager with zero-downtime capability

# ====================================================================
# ðŸ“ˆ MARKET REGIME DETECTION - Use in enhanced_data_pipeline.py
# ====================================================================

def detect_market_regimes():
    """
    Hidden Markov Model for market regime classification
    Adapts trading strategy based on bull/bear/sideways markets
    """
    # Implement 3-state HMM using VIX, yield curve, momentum indicators
    
    # Calculate regime transition probabilities using Baum-Welch algorithm
    
    # Add macroeconomic features: unemployment, inflation, fed funds rate
    
    # Create regime-specific volatility and correlation forecasts
    
    # Adjust position sizing based on regime uncertainty
    
    # Generate regime change alerts with confidence intervals
    
    # Store regime history in PostgreSQL for backtesting
    
    # Return current regime classification with transition probabilities

print("ðŸŽ¯ READY TO USE!")
print("ðŸ’¡ Open VS Code, navigate to the target file, and paste these prompts")
print("ðŸ§  Your enhanced Copilot will generate institutional-grade code!")
print("âš¡ Expected result: Production-ready quantitative trading features!")

# ====================================================================
# ï¿½ PRECISION PROMPTING FOR COPILOT MASTERY
# ====================================================================

def calculate_exact_sentiment_percentages():
    """
    FILE: sentiment_analyzer.py
    CONTEXT: After reddit_comments data loading
    TASK: Calculate precise bullish/bearish/neutral percentages
    INPUT: reddit_comments DataFrame with sentiment_score column
    OUTPUT: Add columns bullish_pct, bearish_pct, neutral_pct
    RULES:
        - bullish: sentiment_score > 0.3 (positive sentiment)
        - bearish: sentiment_score < -0.3 (negative sentiment)  
        - neutral: between -0.3 and 0.3 (mixed/unclear)
    CONSTRAINTS:
        - Use vectorized pandas operations (no loops)
        - Handle empty DataFrames gracefully
        - Return 0.0 instead of NaN for edge cases
    EXAMPLE OUTPUT:
        For 1000 comments: bullish_pct=42.3, bearish_pct=15.8, neutral_pct=41.9
    PERFORMANCE: Process 10K+ comments in <100ms
    """
    # Let Copilot generate vectorized sentiment percentage calculator

def create_multi_ticker_signals():
    """
    FILE: stock_ranking.py  
    CONTEXT: In generate_portfolio_signals() function
    TASK: Create unified buy/sell signals for multiple stocks
    INPUT: Dict of predictions from tft_predictions table
        {"NVDA": {"predicted_return": 0.02, "confidence": 0.95}, ...}
    OUTPUT: Dict with trading actions and position sizes
        {"NVDA": {"action": "STRONG_BUY", "allocation": 0.08, "score": 92.5}}
    LOGIC:
        - STRONG_BUY: predicted_return > 0.02 AND confidence > 0.9
        - BUY: predicted_return > 0.005 AND confidence > 0.7
        - SELL: predicted_return < -0.005 AND confidence > 0.7
        - STRONG_SELL: predicted_return < -0.02 AND confidence > 0.9
        - HOLD: all other conditions
    CONSTRAINTS:
        - Max 10% allocation per stock
        - Max 30% allocation per sector
        - Cash allocation = 1 - sum(allocations)
    RISK_CONTROLS:
        - When VIX > 30: reduce all allocations by 50%
        - When confidence < 0.6: force action = "HOLD"
    """
    # Let Copilot generate multi-ticker signal generator

def implement_real_time_anomaly_detection():
    """
    FILE: anomaly_detector.py
    CONTEXT: Real-time data processing pipeline
    TASK: Detect price and volume anomalies in streaming data
    INPUT: Minute-level candlestick data with volume
        {"ticker": "NVDA", "close": 176.08, "volume": 150000, "timestamp": "..."}
    OUTPUT: Anomaly scores and flags
        {"price_anomaly_score": 85.2, "volume_anomaly_score": 12.1, "flags": ["PRICE_SPIKE"]}
    DETECTION_RULES:
        - PRICE_SPIKE: 5-min return > 3% (score 70-100)
        - VOLUME_SURGE: volume > 3x 20-period average (score 60-90)
        - VOLATILITY_SPIKE: 20-period std > 2x historical (score 50-80)
    CONSTRAINTS:
        - Use rolling statistics (no future data)
        - Process 1000+ tickers in real-time
        - Store anomalies in PostgreSQL anomalies table
    EXCLUSIONS:
        - Skip detection 30min before/after earnings
        - Ignore pre-market/after-hours anomalies
    """
    # Let Copilot generate real-time anomaly detector

def build_options_pricing_engine():
    """
    FILE: options_calculator.py
    CONTEXT: After market data initialization
    TASK: Calculate option prices and Greeks using Black-Scholes
    INPUT: Options parameters
        underlying_price=176.08, strike=180, days_to_expiry=30,
        risk_free_rate=0.045, volatility=0.35, option_type="call"
    OUTPUT: Complete options metrics
        {
            "option_price": 8.42,
            "delta": 0.58, "gamma": 0.032, "theta": -0.15,
            "vega": 0.24, "rho": 0.089,
            "implied_volatility": 0.352
        }
    FORMULAS:
        - Use Black-Scholes-Merton model
        - Include dividend yield adjustments
        - Newton-Raphson for IV calculation
    CONSTRAINTS:
        - Vectorized for options chains (1000+ options)
        - Handle American vs European style
        - Validate inputs (positive prices, valid dates)
    PERFORMANCE:
        - Process full options chain in <500ms
        - Use numpy for mathematical operations
    """
    # Let Copilot generate complete options pricing engine

def create_tax_optimized_rebalancer():
    """
    FILE: tax_optimizer.py
    CONTEXT: Portfolio rebalancing with tax considerations
    TASK: Minimize tax impact while maintaining target allocations
    INPUT: Current positions with cost basis and target weights
        positions = {
            "NVDA": {"shares": 100, "cost_basis": 150.00, "target_weight": 0.12},
            "TSLA": {"shares": 50, "cost_basis": 280.00, "target_weight": 0.08}
        }
    OUTPUT: Tax-optimized trading instructions  
        {
            "trades": [{"ticker": "NVDA", "action": "SELL", "shares": 25, "tax_impact": -850.50}],
            "total_tax_savings": 1250.75,
            "wash_sale_violations": []
        }
    TAX_RULES:
        - Prioritize long-term gains (>365 days holding)
        - Harvest losses before year-end
        - Avoid wash sale violations (30-day rule)
        - Consider state tax implications
    OPTIMIZATION:
        - Minimize total tax liability
        - Maintain portfolio risk profile
        - Respect rebalancing tolerances (Â±2%)
    """
    # Let Copilot generate tax-optimized rebalancer

def implement_dark_pool_analytics():
    """
    FILE: dark_pool_detector.py
    CONTEXT: After Polygon WebSocket connection setup
    TASK: Identify institutional block trades and dark pool activity
    INPUT: Real-time trade data with venue information
        {"ticker": "NVDA", "price": 176.08, "size": 50000, "venue": "D", "timestamp": "..."}
    OUTPUT: Institutional flow indicators
        {
            "block_trade_score": 78.5,
            "dark_pool_ratio": 0.342,
            "institutional_sentiment": "ACCUMULATING",
            "flow_imbalance": 0.156
        }
    DETECTION_LOGIC:
        - Block trades: size > $1M or size > 10x average
        - Dark pool venues: D, J, K, N, X, Y venue codes
        - Institutional flow: rolling 1hr block trade volume
    CALCULATIONS:
        - dark_pool_ratio = dark_volume / total_volume
        - flow_imbalance = (buy_volume - sell_volume) / total_volume
        - block_trade_score = weighted by size and frequency
    CONSTRAINTS:
        - Real-time processing (< 10ms latency)
        - Store signals in PostgreSQL dark_pool_signals table
        - Generate alerts for unusual activity (>2 sigma)
    """
    # Let Copilot generate dark pool analytics engine

def build_earnings_impact_predictor():
    """
    FILE: earnings_predictor.py
    CONTEXT: Before TFT model prediction generation
    TASK: Adjust predictions based on earnings proximity and surprise history
    INPUT: Stock predictions with earnings calendar data
        {
            "ticker": "NVDA", 
            "base_prediction": 0.02,
            "days_to_earnings": 5,
            "earnings_surprise_history": [0.15, -0.08, 0.22, 0.05]
        }
    OUTPUT: Earnings-adjusted predictions with uncertainty
        {
            "adjusted_prediction": 0.025,
            "earnings_volatility_boost": 1.35,
            "confidence_reduction": 0.15,
            "earnings_momentum": "POSITIVE"
        }
    ADJUSTMENT_RULES:
        - 7 days before earnings: increase volatility by 50%
        - Positive surprise history: slight bullish bias (+10%)
        - Negative surprise history: slight bearish bias (-10%)
        - Day of earnings: reduce confidence by 30%
    SURPRISE_MOMENTUM:
        - POSITIVE: 3+ quarters of beats
        - NEGATIVE: 3+ quarters of misses  
        - MIXED: inconsistent pattern
    CONSTRAINTS:
        - Use Polygon earnings calendar API
        - Cache earnings data for performance
        - Handle missing earnings data gracefully
    """
    # Let Copilot generate earnings impact predictor

# ====================================================================
# ðŸš€ MASTER PROMPT EXECUTION GUIDE
# ====================================================================
"""
ðŸŽ¯ PERFECT COPILOT WORKFLOW:

1. PREPARATION:
   - Open VS Code: code enhanced_data_pipeline.py
   - Navigate to target function location
   - Ensure database connections are available

2. PROMPT EXECUTION:
   - Copy ANY prompt above
   - Paste in your target file
   - Press TAB and watch Copilot generate institutional-grade code
   - Expected result: 50-200 lines of production-ready code

3. IMMEDIATE VALIDATION:
   - Test the generated function
   - Check database integration
   - Verify error handling

4. ADVANCED USAGE:
   - Chain prompts for complex workflows
   - Modify constraints for your specific needs
   - Combine multiple prompts for comprehensive solutions

ðŸ† THESE PROMPTS WILL GENERATE:
   âœ… Vectorized pandas operations (10x faster)
   âœ… Production error handling
   âœ… SQL query optimization
   âœ… Financial domain expertise
   âœ… Real-time processing capabilities
   âœ… Tax and regulatory compliance
   âœ… Institutional-grade risk controls

ðŸ’¡ PRO TIP: Use these prompts sequentially to build a complete trading system!

# ====================================================================
# ðŸŒ SYSTEM ARCHITECTURE & ORCHESTRATION PROMPTS
# ====================================================================

def create_master_pipeline_orchestrator():
    """
    FILE: main_pipeline.py
    CONTEXT: Central system coordinator after imports
    TASK: Implement fault-tolerant trading pipeline orchestrator
    INPUT: Market schedule, configuration parameters, database connections
    OUTPUT: Pipeline execution report with success rates and error details
    WORKFLOW:
        1. Check market status (open/closed/pre-market)
        2. Update data sources (OHLCV, sentiment, earnings)
        3. Trigger model retraining if accuracy drops >5%
        4. Generate predictions for all active tickers
        5. Execute trades based on portfolio optimization
        6. Send performance summary to Slack/email
    CONSTRAINTS:
        - Must handle partial failures with checkpoint restart
        - Idempotent operations (safe to re-run)
        - Complete pipeline in <15 minutes during market hours
        - Store execution logs in PostgreSQL pipeline_logs table
    ERROR_HANDLING:
        - Retry failed components 3x with exponential backoff
        - Skip individual stocks that fail, continue with others
        - Send alerts for system-wide failures
    PERFORMANCE:
        - Process 500+ stocks in parallel
        - Use async/await for database operations
        - Implement circuit breakers for external APIs
    """
    # Let Copilot generate complete pipeline orchestrator

def implement_dynamic_config_system():
    """
    FILE: core/config_manager.py
    CONTEXT: Configuration management for live system
    TASK: Create hot-reload configuration system with validation
    INPUT: YAML config files, environment variables, database settings
    OUTPUT: Validated configuration objects with change notifications
    FEATURES:
        - Watch config files for changes using watchdog
        - Validate new configs before applying (schema validation)
        - Notify all system components of config changes
        - Maintain config version history with rollback capability
        - Support environment-specific overrides (dev/staging/prod)
    CONSTRAINTS:
        - Zero-downtime updates during trading hours
        - Configuration changes must be atomic
        - Audit trail of all configuration changes
        - Type-safe configuration objects with validation
    VALIDATION_RULES:
        - Portfolio limits: position_size <= 0.1, sector_limit <= 0.3
        - Risk parameters: stop_loss >= 0.01, take_profit <= 0.1
        - API rate limits: requests_per_second <= 100
    """
    # Let Copilot generate dynamic configuration manager

def build_ml_ops_training_system():
    """
    FILE: ml_ops/train_manager.py
    CONTEXT: Machine learning operations coordinator
    TASK: Implement automated model lifecycle management
    INPUT: Training schedules, model performance metrics, data quality scores
    OUTPUT: Model registry with versioning and deployment status
    TRAINING_STRATEGY:
        - Daily incremental training on new data
        - Weekly full retraining from scratch
        - Emergency retraining when accuracy drops >10%
        - A/B testing between model versions
    MODEL_MANAGEMENT:
        - Version control with MLflow tracking
        - Auto-promote models with >2% accuracy improvement
        - Canary deployment: route 10% traffic to new models
        - Automatic rollback on performance degradation
    CONSTRAINTS:
        - GPU memory optimization for batch processing
        - Distributed training across multiple GPUs
        - Model artifact storage in S3/Azure Blob
        - Performance benchmarking on historical data
    QUALITY_GATES:
        - Minimum accuracy threshold: 55% on validation set
        - Maximum latency: <50ms per prediction
        - Sharpe ratio improvement: >0.1 vs previous model
    """
    # Let Copilot generate ML operations system

def create_realtime_sentiment_momentum():
    """
    FILE: data_modules/sentiment_engine.py
    CONTEXT: Enhanced sentiment analysis with momentum scoring
    TASK: Build real-time sentiment momentum detector
    INPUT: Streaming Reddit comments, historical sentiment baselines
    OUTPUT: Sentiment momentum scores with anomaly flags
        {
            "ticker": "NVDA",
            "sentiment_momentum": 0.42,
            "bullish_acceleration": 0.15,
            "abnormal_spike": True,
            "sector_relative_score": 0.78
        }
    MOMENTUM_CALCULATION:
        - 1-hour sentiment velocity: (current - previous) / time_delta
        - 24-hour sentiment acceleration: velocity change rate
        - Sector-relative scoring: individual vs sector average
        - Volume-weighted sentiment: weight by comment engagement
    ANOMALY_DETECTION:
        - Statistical: sentiment change >3 sigma from mean
        - Contextual: unusual patterns vs historical behavior
        - Cross-asset: detect correlated sentiment spikes
    CONSTRAINTS:
        - Process 10,000+ comments per second
        - Use async processing with message queues
        - Store results in time-series database (InfluxDB)
        - Generate alerts for significant momentum shifts
    """
    # Let Copilot generate sentiment momentum engine

def implement_portfolio_optimization():
    """
    FILE: trading/portfolio_engine.py
    CONTEXT: Advanced portfolio construction with risk management
    TASK: Build Black-Litterman portfolio optimizer with sentiment integration
    INPUT: Model predictions, sentiment scores, market equilibrium returns
    OUTPUT: Optimal portfolio weights with risk metrics
        {
            "weights": {"NVDA": 0.08, "TSLA": 0.05, "CASH": 0.15},
            "expected_return": 0.12,
            "portfolio_volatility": 0.18,
            "sharpe_ratio": 0.85,
            "max_drawdown": -0.08
        }
    OPTIMIZATION_METHOD:
        - Black-Litterman framework with market equilibrium
        - Sentiment confidence as view certainty
        - Risk parity adjustments for volatility
        - Transaction cost minimization
    CONSTRAINTS:
        - Maximum position size: 5% per stock
        - Maximum sector exposure: 25% per sector
        - Minimum diversification: 20+ positions
        - Maximum portfolio beta: 1.2
    RISK_CONTROLS:
        - VaR calculation at 95% confidence
        - Stress testing against historical scenarios
        - Correlation limits between positions
        - Dynamic hedging with VIX options
    """
    # Let Copilot generate portfolio optimization engine

def build_system_anomaly_detector():
    """
    FILE: monitoring/anomaly_detector.py
    CONTEXT: Real-time system health monitoring
    TASK: Create multivariate anomaly detection for system reliability
    INPUT: System metrics, prediction accuracy, market data quality
    OUTPUT: Anomaly scores and diagnostic reports
        {
            "overall_health": 92.5,
            "data_drift_score": 15.2,
            "model_drift_score": 8.7,
            "system_latency_score": 5.1,
            "alerts": ["High prediction variance detected"]
        }
    DETECTION_CATEGORIES:
        - Data drift: feature distribution changes
        - Concept drift: prediction accuracy degradation
        - System anomalies: latency spikes, memory leaks
        - Market regime changes: volatility shifts
    ALGORITHMS:
        - Isolation Forest for multivariate detection
        - LSTM autoencoder for time-series anomalies
        - Statistical process control for system metrics
        - Kolmogorov-Smirnov test for distribution drift
    CONSTRAINTS:
        - Detection latency <100ms
        - False positive rate <2%
        - Store anomaly history for trend analysis
        - Integration with alerting systems (PagerDuty)
    """
    # Let Copilot generate system anomaly detector

def create_cicd_deployment_pipeline():
    """
    FILE: .github/workflows/deploy_production.yml
    CONTEXT: GitHub Actions CI/CD pipeline
    TASK: Build production deployment workflow with validation
    INPUT: Code changes, model artifacts, configuration updates
    OUTPUT: Deployed system with performance validation
    PIPELINE_STAGES:
        1. Code quality: linting, type checking, security scan
        2. Unit tests: 90%+ coverage requirement
        3. Integration tests: database, APIs, model inference
        4. Backtesting: validate on 3 market regimes
        5. Build Docker images with GPU support
        6. Deploy to Kubernetes cluster (blue-green)
        7. Canary testing with 5% traffic
        8. Full deployment or automatic rollback
    VALIDATION_GATES:
        - Backtesting Sharpe ratio >0.6
        - Maximum drawdown <15%
        - Model latency <50ms p99
        - System uptime >99.9%
    DEPLOYMENT_STRATEGY:
        - Blue-green deployment for zero downtime
        - Feature flags for gradual rollouts
        - Automated rollback on performance degradation
        - Database migrations with rollback scripts
    """
    # Let Copilot generate CI/CD deployment pipeline

# ====================================================================
# ðŸ”¬ ADVANCED TESTING & VALIDATION PROMPTS
# ====================================================================

def build_market_regime_validator():
    """
    FILE: tests/regime_validation.py
    CONTEXT: Comprehensive backtesting across market conditions
    TASK: Create market regime-specific validation system
    INPUT: Historical data 2019-2024, model predictions, market indicators
    OUTPUT: Performance metrics by market regime
        {
            "bull_market": {"sharpe": 0.85, "max_dd": -0.08, "win_rate": 0.62},
            "volatile_market": {"sharpe": 0.45, "max_dd": -0.15, "win_rate": 0.48},
            "bear_market": {"sharpe": 0.25, "max_dd": -0.22, "win_rate": 0.35}
        }
    REGIME_CLASSIFICATION:
        - Bull market: VIX <15, positive 3-month returns
        - Volatile market: VIX 20-30, high correlation breakdown
        - Bear market: VIX >35, negative 6-month returns
        - Transition periods: regime uncertainty >0.3
    VALIDATION_METRICS:
        - Risk-adjusted returns (Sharpe, Sortino ratio)
        - Maximum drawdown and recovery time
        - Win rate and profit factor
        - Tail risk (VaR, CVaR at 95% confidence)
    CONSTRAINTS:
        - Test on 2000+ trading days
        - Include transaction costs and slippage
        - Account for market impact on large trades
        - Validate statistical significance (t-tests)
    """
    # Let Copilot generate market regime validator

def implement_stress_testing_framework():
    """
    FILE: tests/stress_tests.py
    CONTEXT: Risk scenario analysis for portfolio robustness
    TASK: Build comprehensive stress testing framework
    INPUT: Portfolio positions, historical shock scenarios, correlation matrices
    OUTPUT: Stress test results with risk decomposition
        {
            "scenario": "2020_covid_crash",
            "portfolio_loss": -0.28,
            "sector_breakdown": {"tech": -0.35, "finance": -0.15},
            "recovery_days": 45,
            "var_breach": True
        }
    STRESS_SCENARIOS:
        - Historical: 2008 crisis, COVID crash, dot-com bubble
        - Hypothetical: interest rate shocks, sector rotation
        - Monte Carlo: 10,000 random market scenarios
        - Tail events: 99th percentile worst-case outcomes
    ANALYSIS_FRAMEWORK:
        - Factor decomposition (market, sector, stock-specific)
        - Time-varying correlations during stress
        - Liquidity risk assessment
        - Margin call and forced liquidation scenarios
    CONSTRAINTS:
        - Run 1000+ scenarios in <5 minutes
        - Include second-order effects (volatility feedback)
        - Account for market microstructure changes
        - Generate actionable hedging recommendations
    """
    # Let Copilot generate stress testing framework

# ====================================================================
# ðŸš€ DEPLOYMENT & SCALING PROMPTS  
# ====================================================================

def create_kubernetes_deployment():
    """
    FILE: k8s/trading-system.yaml
    CONTEXT: Production Kubernetes deployment configuration
    TASK: Build scalable, fault-tolerant K8s deployment
    INPUT: Docker images, resource requirements, scaling policies
    OUTPUT: Complete Kubernetes manifests with monitoring
    COMPONENTS:
        - TFT Model Service: GPU-enabled pods with model serving
        - Data Pipeline: CPU-intensive ETL workers
        - Trading Engine: Low-latency execution service
        - Database: PostgreSQL with read replicas
        - Redis: Caching and message queuing
        - Monitoring: Prometheus, Grafana, Jaeger tracing
    SCALING_STRATEGY:
        - Horizontal Pod Autoscaler based on CPU/memory/custom metrics
        - Vertical Pod Autoscaler for right-sizing
        - Cluster Autoscaler for node scaling
        - GPU node pools for ML workloads
    RELIABILITY:
        - Pod disruption budgets for maintenance
        - Health checks and readiness probes
        - Circuit breakers and retry policies
        - Cross-zone deployment for high availability
    """
    # Let Copilot generate Kubernetes deployment

def implement_monitoring_observability():
    """
    FILE: monitoring/observability.py
    CONTEXT: Complete system observability stack
    TASK: Build comprehensive monitoring and alerting system
    INPUT: Application metrics, system metrics, business metrics
    OUTPUT: Dashboards, alerts, and SLA monitoring
    METRICS_CATEGORIES:
        - Business: P&L, Sharpe ratio, win rate, drawdown
        - Technical: latency, throughput, error rates, uptime
        - Infrastructure: CPU, memory, disk, network utilization
        - Model: prediction accuracy, drift detection, feature importance
    DASHBOARDS:
        - Executive: high-level P&L and risk metrics
        - Operations: system health and performance
        - Trading: real-time positions and market data
        - Development: model performance and data quality
    ALERTING_RULES:
        - Critical: system down, trading halted, model failure
        - Warning: performance degradation, data delays
        - Info: deployment success, model updates
    SLA_MONITORING:
        - Uptime: 99.9% availability during market hours
        - Latency: <100ms p99 for trading operations
        - Accuracy: Model predictions within 2% of targets
    """
    # Let Copilot generate monitoring system

# ====================================================================
# ðŸ’¡ PROMPT CHAINING WORKFLOWS
# ====================================================================
"""
ðŸ”„ SEQUENTIAL PROMPT EXECUTION FOR COMPLETE SYSTEM:

Phase 1 - Foundation:
1. create_master_pipeline_orchestrator()
2. implement_dynamic_config_system()  
3. build_ml_ops_training_system()

Phase 2 - Intelligence:
4. create_realtime_sentiment_momentum()
5. implement_portfolio_optimization()
6. build_system_anomaly_detector()

Phase 3 - Production:
7. create_cicd_deployment_pipeline()
8. build_market_regime_validator()
9. create_kubernetes_deployment()

ðŸŽ¯ EXPECTED OUTCOMES:
- Complete institutional trading platform
- 500+ stocks processed in parallel
- <50ms prediction latency
- 99.9% uptime SLA
- Automated model lifecycle
- Real-time risk management
- Production monitoring stack

ðŸ’ª POWER USER TIPS:
1. Combine prompts: Use sentiment + portfolio optimization together
2. Customize constraints: Adjust position limits, timeframes
3. Add domain knowledge: Include specific financial requirements
4. Test incrementally: Validate each component before integration

# ====================================================================
# ðŸ“‹ COMPLETE SYSTEM ARCHITECTURE GENERATOR
# ====================================================================

def generate_complete_project_summary():
    """
    FILE: docs/SYSTEM_ARCHITECTURE.md
    CONTEXT: Complete system documentation generator
    TASK: Generate comprehensive project summary with all components
    INPUT: System requirements, performance targets, deployment specs
    OUTPUT: Complete architectural documentation with diagrams
    
    SYSTEM_COMPONENTS:
        - PostgreSQL Core: market_data, reddit_comments, sentiment_metrics, 
          model_predictions, trade_executions tables
        - Microservices: Data Ingestion, TFT Training, Prediction Engine, 
          Trade Execution services
        - Real-time Pipeline: Apache Kafka streaming processor
        - Monitoring Stack: Prometheus/Grafana/AlertManager
        
    ARCHITECTURE_FEATURES:
        - Sentiment Analysis Engine with bullish/bearish classification
        - TFT Prediction System with multi-horizon forecasting
        - Trading Execution with risk-managed position sizing
        - Broker Integration: Alpaca/Polygon APIs
        
    DATA_PIPELINE:
        ```mermaid
        graph LR
        A[Market Data] --> B[Data Normalization]
        C[Reddit Comments] --> D[Sentiment Scoring]
        B --> E[Feature Engineering]
        D --> E
        E --> F[TFT Model]
        F --> G[Prediction Store]
        ```
        
    PERFORMANCE_TARGETS:
        - Sentiment Analysis Processing: < 100ms latency
        - Model Training MAPE: < 2.5% validation error
        - Trade Execution Sharpe Ratio: > 1.8 target
        - System Throughput: 10k messages/second
        
    DEPLOYMENT_ENVIRONMENT:
        - Kubernetes cluster (AWS EKS)
        - GPU-optimized nodes for training
        - Serverless prediction endpoints
        - TimescaleDB for time-series data
        
    INNOVATION_HIGHLIGHTS:
        - Hybrid fundamental/technical/sentiment features
        - Adaptive model retraining triggers
        - Explainable AI trade rationale
        - Volatility-adaptive position sizing
        
    CONSTRAINTS:
        - Generate Mermaid.js diagrams
        - Include performance metrics table
        - Create deployment topology
        - Add innovation roadmap
    """
    # Let Copilot generate complete system architecture document

def create_data_pipeline_specifications():
    """
    FILE: docs/DATA_PIPELINE.md
    CONTEXT: Detailed data flow documentation
    TASK: Create comprehensive data pipeline specification
    INPUT: Data sources, processing stages, storage requirements
    OUTPUT: Technical data flow documentation with code examples
    
    DATA_SOURCES:
        - Polygon.io: Real-time market data (OHLCV, trades, quotes)
        - Reddit API: Comments and submissions from financial subreddits
        - Economic Calendar: Earnings dates, economic indicators
        - Alternative Data: Satellite imagery, patent filings, web scraping
        
    PROCESSING_STAGES:
        1. Data Ingestion: Multi-threaded collectors with rate limiting
        2. Data Normalization: Schema validation and type conversion
        3. Feature Engineering: Technical indicators and sentiment scoring
        4. Model Training: TFT with attention mechanisms
        5. Prediction Generation: Multi-horizon forecasting
        6. Signal Processing: Trade signal generation with risk controls
        
    STORAGE_ARCHITECTURE:
        - Raw Data Lake: S3/MinIO with Parquet format
        - Operational Database: PostgreSQL with TimescaleDB
        - Model Artifacts: MLflow registry with version control
        - Cache Layer: Redis for real-time data
        
    EXAMPLE_PIPELINE_CODE:
        ```python
        async def process_market_data(ticker):
            # Fetch raw data from Polygon
            raw_data = await polygon_client.get_ohlcv(ticker)
            
            # Apply technical indicators
            features = calculate_technical_indicators(raw_data)
            
            # Generate predictions
            predictions = tft_model.predict(features)
            
            # Store results
            await db.store_predictions(ticker, predictions)
        ```
        
    PERFORMANCE_REQUIREMENTS:
        - Data ingestion: 50k records/second
        - Processing latency: <200ms end-to-end
        - Storage capacity: 10TB+ with 99.9% availability
        - Backup recovery: <1 hour RTO, <15 minutes RPO
        
    CONSTRAINTS:
        - Include detailed code examples
        - Specify error handling strategies
        - Document monitoring and alerting
        - Add capacity planning guidelines
    """
    # Let Copilot generate detailed pipeline documentation

def build_deployment_architecture():
    """
    FILE: k8s/DEPLOYMENT_GUIDE.md
    CONTEXT: Production deployment specifications
    TASK: Create comprehensive deployment guide with Infrastructure as Code
    INPUT: System requirements, scaling policies, security configurations
    OUTPUT: Complete deployment documentation with Terraform/Helm charts
    
    INFRASTRUCTURE_COMPONENTS:
        - EKS Cluster: Multi-AZ with auto-scaling node groups
        - GPU Nodes: g4dn.xlarge for model training workloads
        - Database: RDS PostgreSQL with read replicas
        - Message Queue: Amazon MSK (Kafka) for streaming
        - Storage: EFS for shared model artifacts
        - Monitoring: CloudWatch + Prometheus stack
        
    KUBERNETES_SERVICES:
        - Data Ingestion: Deployment with HPA (2-10 replicas)
        - TFT Training: CronJob with GPU node selector
        - Prediction API: Deployment with load balancer
        - Trade Execution: StatefulSet with persistent volumes
        - Monitoring: DaemonSet for metrics collection
        
    SCALING_POLICIES:
        - CPU-based: Scale at 70% utilization
        - Memory-based: Scale at 80% utilization
        - Custom metrics: Queue depth, prediction latency
        - Cluster autoscaler: Add nodes when pending pods > 30s
        
    SECURITY_CONFIGURATION:
        - IAM roles with least privilege principle
        - Network policies for pod-to-pod communication
        - Secrets management with AWS Secrets Manager
        - TLS encryption for all external communications
        
    EXAMPLE_HELM_VALUES:
        ```yaml
        tftService:
          replicaCount: 3
          resources:
            requests:
              cpu: "2"
              memory: "8Gi"
            limits:
              cpu: "4"
              memory: "16Gi"
          nodeSelector:
            instance-type: "gpu"
        ```
        
    MONITORING_SETUP:
        - SLI/SLO definitions for each service
        - Grafana dashboards for business metrics
        - PagerDuty integration for critical alerts
        - Log aggregation with ELK stack
        
    CONSTRAINTS:
        - Generate production-ready Helm charts
        - Include security best practices
        - Add disaster recovery procedures
        - Document troubleshooting guides
    """
    # Let Copilot generate deployment architecture guide

def create_performance_benchmarking_suite():
    """
    FILE: tests/performance_benchmarks.py
    CONTEXT: Comprehensive performance testing framework
    TASK: Build performance benchmarking and load testing suite
    INPUT: System components, performance targets, test scenarios
    OUTPUT: Automated performance test suite with metrics collection
    
    BENCHMARK_CATEGORIES:
        - Latency Tests: End-to-end response times
        - Throughput Tests: Maximum sustained load
        - Stress Tests: System behavior under extreme conditions
        - Endurance Tests: Long-running stability validation
        - Scalability Tests: Performance across different loads
        
    TEST_SCENARIOS:
        - Market Open: 10x normal load for 30 minutes
        - Earnings Season: 5x normal load with sentiment spikes
        - Flash Crash: Extreme volatility with rapid updates
        - Weekend Processing: Batch model retraining
        - Network Partition: Failure resilience testing
        
    PERFORMANCE_TARGETS:
        Component         | Metric              | Target      | SLA
        Data Ingestion    | Throughput          | 50k msg/s   | 99.9%
        Sentiment Engine  | P99 Latency         | <100ms      | 99.5%
        TFT Prediction    | Batch Processing    | 1k pred/s   | 99.0%
        Trade Execution   | Order Latency       | <50ms       | 99.9%
        Database Queries  | P95 Response        | <10ms       | 99.5%
        
    LOAD_TESTING_CODE:
        ```python
        async def benchmark_sentiment_engine():
            # Generate synthetic load
            test_data = create_sentiment_test_data(10000)
            
            # Measure performance
            start_time = time.time()
            results = await process_sentiment_batch(test_data)
            duration = time.time() - start_time
            
            # Validate SLA
            throughput = len(results) / duration
            assert throughput > 5000  # 5k comments/second
        ```
        
    MONITORING_INTEGRATION:
        - Custom CloudWatch metrics for each test
        - Grafana dashboard for performance trends
        - Automated alerting on SLA violations
        - Historical performance tracking
        
    CONSTRAINTS:
        - Automated execution in CI/CD pipeline
        - Realistic data generation for tests
        - Comprehensive failure scenario coverage
        - Performance regression detection
    """
    # Let Copilot generate performance benchmarking suite

def generate_innovation_roadmap():
    """
    FILE: docs/INNOVATION_ROADMAP.md
    CONTEXT: Future enhancements and advanced features
    TASK: Create technical innovation roadmap with implementation priorities
    INPUT: Current system capabilities, market requirements, technology trends
    OUTPUT: Prioritized roadmap with technical specifications
    
    CURRENT_CAPABILITIES:
        - Real-time sentiment analysis with 85% accuracy
        - TFT predictions with 2.3% MAPE on validation
        - Automated trading with 1.9 Sharpe ratio
        - 99.8% system uptime with <100ms latency
        
    PHASE_1_ENHANCEMENTS (Q1 2025):
        - Multi-asset correlation analysis
        - Options flow integration for directional bias
        - Enhanced risk management with VaR calculations
        - Real-time model retraining triggers
        
    PHASE_2_INNOVATIONS (Q2-Q3 2025):
        - Transformer-based market regime detection
        - Alternative data fusion (satellite, patent, hiring)
        - Reinforcement learning for position sizing
        - Explainable AI for trade rationale
        
    PHASE_3_ADVANCED (Q4 2025):
        - Multi-modal learning (text, image, numerical)
        - Graph neural networks for sector relationships
        - Quantum-inspired optimization algorithms
        - Automated strategy discovery via genetic programming
        
    TECHNICAL_SPECIFICATIONS:
        Feature                 | Technology Stack      | Expected Impact
        Market Regime Detection | HMM + Transformer     | 15% accuracy boost
        Options Flow Analysis   | Real-time streaming   | 25% alpha increase  
        Alt Data Fusion        | Multi-modal ML        | 10% Sharpe improvement
        RL Position Sizing     | PPO + risk constraints| 20% drawdown reduction
        
    IMPLEMENTATION_PRIORITIES:
        1. HIGH: Options flow (immediate alpha generation)
        2. MEDIUM: Market regime (strategy adaptation)
        3. LOW: Quantum optimization (research phase)
        
    RESOURCE_REQUIREMENTS:
        - Data Scientists: 2-3 FTE for advanced ML
        - Infrastructure: Additional GPU compute for training
        - Data Costs: $50k/year for alternative data feeds
        - Timeline: 18 months for complete roadmap
        
    CONSTRAINTS:
        - Maintain backward compatibility
        - Preserve system performance
        - Ensure regulatory compliance
        - Document all innovations thoroughly
    """
    # Let Copilot generate comprehensive innovation roadmap

# ====================================================================
# ï¿½ï¸ MICROSERVICE ARCHITECTURE PROMPTS
# ====================================================================

def create_microservice_data_ingestion():
    """
    FILE: services/data-ingestion/main.py
    CONTEXT: Microservice for real-time data collection and publishing
    TASK: Build scalable data ingestion service with Kafka publishing
    INPUT: API configurations, data source endpoints, message schemas
    OUTPUT: Clean, validated data streams to Kafka topics
    
    SERVICE_RESPONSIBILITIES:
        - Polygon.io market data collection (OHLCV, trades, quotes)
        - Reddit API scraping with rate limiting
        - Data validation and schema enforcement
        - Message publishing to Kafka topics
        - Health checks and metrics collection
    
    KAFKA_TOPICS:
        - market-data: Raw OHLCV data
        - reddit-comments: Sentiment analysis input
        - earnings-calendar: Fundamental events
        - system-health: Service monitoring
    
    API_ENDPOINTS:
        - GET /health: Service health status
        - GET /metrics: Prometheus metrics
        - POST /trigger/{data_type}: Manual data collection
        - GET /status/{ticker}: Data freshness status
    
    PERFORMANCE_TARGETS:
        - Throughput: 10k messages/second
        - Latency: <50ms message publishing
        - Availability: 99.9% uptime SLA
        - Error rate: <0.1% failed messages
    
    CONSTRAINTS:
        - Use async/await for concurrent API calls
        - Implement circuit breakers for external APIs
        - Dead letter queues for failed messages
        - Comprehensive logging and monitoring
    """
    # Let Copilot generate microservice data ingestion service

def create_microservice_sentiment_engine():
    """
    FILE: services/sentiment-engine/main.py
    CONTEXT: Dedicated sentiment analysis microservice
    TASK: Build high-throughput sentiment processing service
    INPUT: Reddit comments from Kafka, historical sentiment baselines
    OUTPUT: Sentiment scores and momentum indicators to Kafka
    
    SERVICE_FEATURES:
        - Real-time sentiment scoring with transformers
        - Momentum calculation and anomaly detection
        - Sector-relative sentiment analysis
        - Caching layer with Redis for performance
    
    MESSAGE_PROCESSING:
        - Consumer: reddit-comments topic
        - Producer: sentiment-scores topic
        - Batch processing: 1000 messages/batch
        - Parallel processing: 4 worker threads
    
    API_ENDPOINTS:
        - POST /analyze: Single comment analysis
        - POST /batch: Bulk sentiment analysis
        - GET /sentiment/{ticker}: Current sentiment status
        - GET /momentum/{ticker}: Sentiment momentum indicators
    
    CACHING_STRATEGY:
        - Redis cache for model predictions (TTL: 5 minutes)
        - Sentiment history cache (TTL: 1 hour)
        - Model artifacts cache (persistent)
    
    CONSTRAINTS:
        - Stateless design for horizontal scaling
        - GPU optimization for transformer models
        - Sub-100ms processing latency target
        - Graceful degradation during peak loads
    """
    # Let Copilot generate microservice sentiment analysis engine

def create_microservice_tft_predictor():
    """
    FILE: services/tft-predictor/main.py
    CONTEXT: TFT model inference and training microservice
    TASK: Build GPU-optimized prediction service with model lifecycle
    INPUT: Feature vectors, training triggers, model parameters
    OUTPUT: Multi-horizon predictions with confidence intervals
    
    SERVICE_CAPABILITIES:
        - Real-time TFT inference with <50ms latency
        - Automated model retraining and versioning
        - A/B testing between model versions
        - Model performance monitoring and drift detection
    
    PREDICTION_PIPELINE:
        - Feature engineering from market data
        - Multi-horizon forecasting (1h, 4h, 24h)
        - Confidence interval calculation
        - Prediction quality scoring
    
    MODEL_LIFECYCLE:
        - Daily incremental training
        - Weekly full retraining
        - Emergency retraining on accuracy drops
        - Blue-green deployment for model updates
    
    API_ENDPOINTS:
        - POST /predict: Single ticker prediction
        - POST /predict/batch: Multi-ticker predictions
        - POST /train: Trigger model training
        - GET /model/status: Current model version info
        - GET /model/performance: Accuracy metrics
    
    GPU_OPTIMIZATION:
        - CUDA memory management
        - Mixed-precision training
        - Batch inference optimization
        - Model quantization for deployment
    
    CONSTRAINTS:
        - Containerized with NVIDIA runtime
        - Model versioning with MLflow
        - Rolling deployments without downtime
        - Comprehensive model monitoring
    """
    # Let Copilot generate TFT prediction microservice

def create_microservice_trading_engine():
    """
    FILE: services/trading-engine/main.py
    CONTEXT: Order execution and risk management microservice
    TASK: Build institutional-grade trading execution service
    INPUT: Trading signals, portfolio state, risk parameters
    OUTPUT: Order executions, fills, and P&L attribution
    
    TRADING_FEATURES:
        - Multi-broker integration (Alpaca, Interactive Brokers)
        - Real-time position tracking and P&L calculation
        - Risk controls and compliance monitoring
        - Smart order routing and execution algorithms
    
    RISK_MANAGEMENT:
        - Position sizing with Kelly criterion
        - Sector and stock concentration limits
        - Stop-loss and take-profit automation
        - Margin and buying power monitoring
    
    ORDER_TYPES:
        - Market orders with slippage protection
        - Limit orders with time-in-force
        - Stop orders and trailing stops
        - Bracket orders for risk management
    
    API_ENDPOINTS:
        - POST /orders: Place new order
        - GET /positions: Current positions
        - GET /orders/{id}: Order status
        - POST /risk/check: Pre-trade risk validation
        - GET /pnl: Portfolio P&L summary
    
    COMPLIANCE_FEATURES:
        - Trade reporting and audit trails
        - Tax lot tracking and optimization
        - Regulatory compliance checks
        - Best execution monitoring
    
    CONSTRAINTS:
        - Sub-50ms order execution latency
        - 99.99% uptime requirement
        - End-to-end encryption for sensitive data
        - Disaster recovery and failover capability
    """
    # Let Copilot generate trading engine microservice

def create_microservice_orchestrator():
    """
    FILE: services/orchestrator/main.py
    CONTEXT: Central coordination service for microservice workflow
    TASK: Build event-driven orchestration with saga pattern
    INPUT: Market events, service health, configuration updates
    OUTPUT: Coordinated workflows and service management
    
    ORCHESTRATION_PATTERNS:
        - Event-driven architecture with Kafka
        - Saga pattern for distributed transactions
        - Circuit breaker for service failures
        - Retry policies with exponential backoff
    
    WORKFLOW_MANAGEMENT:
        - Market open/close procedures
        - Data pipeline coordination
        - Model training scheduling
        - Portfolio rebalancing triggers
    
    SERVICE_DISCOVERY:
        - Kubernetes service discovery
        - Health check aggregation
        - Load balancing configuration
        - Service mesh integration (Istio)
    
    API_GATEWAY:
        - Request routing and load balancing
        - Authentication and authorization
        - Rate limiting and throttling
        - API versioning and documentation
    
    MONITORING_INTEGRATION:
        - Distributed tracing with Jaeger
        - Metrics aggregation with Prometheus
        - Log aggregation with ELK stack
        - Alerting with PagerDuty integration
    
    CONSTRAINTS:
        - Event-driven design with eventual consistency
        - Idempotent operations for reliability
        - Graceful degradation during failures
        - Comprehensive observability stack
    """
    # Let Copilot generate microservice orchestration service

def create_microservice_deployment_config():
    """
    FILE: k8s/microservices-deployment.yaml
    CONTEXT: Kubernetes deployment for microservice architecture
    TASK: Create complete K8s manifests for all microservices
    INPUT: Service requirements, scaling policies, networking configs
    OUTPUT: Production-ready Kubernetes deployment manifests
    
    SERVICES_DEPLOYMENT:
        - Data Ingestion: 3 replicas, CPU-optimized
        - Sentiment Engine: 2 replicas, GPU-enabled
        - TFT Predictor: 2 replicas, GPU-optimized
        - Trading Engine: 3 replicas, low-latency network
        - Orchestrator: 2 replicas, high availability
    
    NETWORKING_CONFIGURATION:
        - Service mesh with Istio
        - Network policies for security
        - Load balancers for external access
        - Internal service discovery
    
    SCALING_POLICIES:
        - HPA based on CPU, memory, and custom metrics
        - VPA for right-sizing containers
        - Cluster autoscaler for node scaling
        - Pod disruption budgets for maintenance
    
    STORAGE_CONFIGURATION:
        - PostgreSQL with read replicas
        - Redis cluster for caching
        - Kafka cluster for messaging
        - Persistent volumes for model artifacts
    
    MONITORING_STACK:
        - Prometheus for metrics collection
        - Grafana for visualization
        - Jaeger for distributed tracing
        - ELK stack for log aggregation
    
    CONSTRAINTS:
        - Multi-AZ deployment for HA
        - Resource quotas and limits
        - Security policies and RBAC
        - Backup and disaster recovery
    """
    # Let Copilot generate microservice Kubernetes deployment

# ====================================================================
# ï¿½ðŸŽ¯ MASTER PROJECT EXECUTION WORKFLOW
# ====================================================================
"""
ðŸš€ MICROSERVICE TFT SYSTEM IMPLEMENTATION GUIDE
==============================================

PHASE 1: FOUNDATION (Week 1-2)
1. create_microservice_deployment_config() â†’ k8s/microservices-deployment.yaml
2. create_microservice_orchestrator() â†’ services/orchestrator/main.py
3. generate_complete_project_summary() â†’ docs/SYSTEM_ARCHITECTURE.md

PHASE 2: CORE SERVICES (Week 3-6)
4. create_microservice_data_ingestion() â†’ services/data-ingestion/main.py
5. create_microservice_sentiment_engine() â†’ services/sentiment-engine/main.py
6. create_microservice_tft_predictor() â†’ services/tft-predictor/main.py

PHASE 3: TRADING SERVICES (Week 7-10)
7. create_microservice_trading_engine() â†’ services/trading-engine/main.py
8. implement_portfolio_optimization() â†’ services/portfolio-engine/main.py
9. create_tax_optimized_rebalancer() â†’ services/tax-optimizer/main.py

ðŸŽ¯ MICROSERVICE ARCHITECTURE BENEFITS:
âœ… Independent scaling per service
âœ… Technology flexibility (Python/Go/Rust)
âœ… Fault isolation and resilience
âœ… Team autonomy and faster development
âœ… Resource optimization (GPU only for ML)

ðŸ’¡ DEPLOYMENT STRATEGY:
- Container orchestration with Kubernetes
- Service mesh for inter-service communication
- Event-driven architecture with Kafka
- Observability with Prometheus/Grafana/Jaeger

ðŸ† EXPECTED PERFORMANCE:
- Horizontal scaling: 10x throughput capacity
- Fault tolerance: 99.99% availability
- Development velocity: 3x faster feature delivery
- Resource efficiency: 40% cost reduction

MONOLITH â†’ MICROSERVICE MIGRATION:
1. Extract data ingestion first (lowest risk)
2. Migrate sentiment analysis (stateless)
3. Split TFT prediction service (GPU isolation)
4. Separate trading engine (compliance isolation)
5. Add orchestration service (workflow coordination)

PHASE 2: CORE IMPLEMENTATION (Week 3-6)
4. calculate_exact_sentiment_percentages() â†’ sentiment_analyzer.py
5. create_multi_ticker_signals() â†’ stock_ranking.py
6. implement_portfolio_optimization() â†’ trading/portfolio_engine.py
7. build_system_anomaly_detector() â†’ monitoring/anomaly_detector.py

PHASE 3: ADVANCED FEATURES (Week 7-10)
8. build_options_pricing_engine() â†’ options_calculator.py
9. implement_dark_pool_analytics() â†’ dark_pool_detector.py
10. create_tax_optimized_rebalancer() â†’ tax_optimizer.py
11. build_earnings_impact_predictor() â†’ earnings_predictor.py

PHASE 4: PRODUCTION DEPLOYMENT (Week 11-12)
12. create_kubernetes_deployment() â†’ k8s/trading-system.yaml
13. implement_monitoring_observability() â†’ monitoring/observability.py
14. create_performance_benchmarking_suite() â†’ tests/performance_benchmarks.py
15. generate_innovation_roadmap() â†’ docs/INNOVATION_ROADMAP.md

ðŸŽ¯ EXPECTED DELIVERABLES:
âœ… Complete system architecture documentation
âœ… Production-ready Kubernetes deployment
âœ… Real-time sentiment momentum engine
âœ… Portfolio optimization with Black-Litterman
âœ… Comprehensive monitoring and alerting
âœ… Performance benchmarking suite
âœ… Innovation roadmap for future enhancements

ðŸ’¡ SUCCESS METRICS:
- System Uptime: 99.9%+ during market hours
- Prediction Latency: <50ms per ticker
- Trading Performance: Sharpe ratio >1.5
- Data Processing: 50k+ records/second
- Model Accuracy: <2.5% MAPE validation error

ðŸ† INSTITUTIONAL-GRADE FEATURES:
- Multi-asset correlation analysis
- Real-time anomaly detection
- Risk-managed portfolio construction
- Tax-optimized trading strategies
- Explainable AI for trade decisions
- Production monitoring stack
"""
